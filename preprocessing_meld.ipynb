{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_sent_emo.csv', index_col=0)\n",
    "df_dev = pd.read_csv('dev_sent_emo.csv', index_col=0)\n",
    "df_test = pd.read_csv('test_sent_emo.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sr No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>also I was the point person on my companys tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You mustve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So lets talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10474</th>\n",
       "      <td>You or me?</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:48,173</td>\n",
       "      <td>00:00:50,799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10475</th>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:51,009</td>\n",
       "      <td>00:00:53,594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10476</th>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>Joey</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>1038</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:00,518</td>\n",
       "      <td>00:01:03,520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10477</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>All</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:05,398</td>\n",
       "      <td>00:01:07,274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10478</th>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1038</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:08,401</td>\n",
       "      <td>00:01:12,071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Utterance          Speaker  \\\n",
       "Sr No.                                                                       \n",
       "1       also I was the point person on my companys tr...         Chandler   \n",
       "2                        You mustve had your hands full.  The Interviewer   \n",
       "3                                 That I did. That I did.         Chandler   \n",
       "4           So lets talk a little bit about your duties.  The Interviewer   \n",
       "5                                  My duties?  All right.         Chandler   \n",
       "...                                                   ...              ...   \n",
       "10474                                          You or me?         Chandler   \n",
       "10475   I got it. Uh, Joey, women don't have Adam's ap...             Ross   \n",
       "10476                You guys are messing with me, right?             Joey   \n",
       "10477                                               Yeah.              All   \n",
       "10478   That was a good one. For a second there, I was...             Joey   \n",
       "\n",
       "         Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "Sr No.                                                                   \n",
       "1        neutral   neutral            0             0       8       21   \n",
       "2        neutral   neutral            0             1       8       21   \n",
       "3        neutral   neutral            0             2       8       21   \n",
       "4        neutral   neutral            0             3       8       21   \n",
       "5       surprise  positive            0             4       8       21   \n",
       "...          ...       ...          ...           ...     ...      ...   \n",
       "10474    neutral   neutral         1038            13       2        3   \n",
       "10475    neutral   neutral         1038            14       2        3   \n",
       "10476   surprise  positive         1038            15       2        3   \n",
       "10477    neutral   neutral         1038            16       2        3   \n",
       "10478        joy  positive         1038            17       2        3   \n",
       "\n",
       "           StartTime       EndTime  \n",
       "Sr No.                              \n",
       "1       00:16:16,059  00:16:21,731  \n",
       "2       00:16:21,940  00:16:23,442  \n",
       "3       00:16:23,442  00:16:26,389  \n",
       "4       00:16:26,820  00:16:29,572  \n",
       "5       00:16:34,452  00:16:40,917  \n",
       "...              ...           ...  \n",
       "10474   00:00:48,173  00:00:50,799  \n",
       "10475   00:00:51,009  00:00:53,594  \n",
       "10476   00:01:00,518  00:01:03,520  \n",
       "10477   00:01:05,398  00:01:07,274  \n",
       "10478   00:01:08,401  00:01:12,071  \n",
       "\n",
       "[9989 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed(df):\n",
    "    global emotion_dict, model, tokenizer\n",
    "    df_group = df.groupby('Dialogue_ID')\n",
    "    \n",
    "    dataset = []\n",
    "    for name, group in df_group:\n",
    "        utterance = group['Utterance'].values\n",
    "        emotion = group['Emotion'].values\n",
    "        startTime = group['StartTime'].values\n",
    "        endTime = group['EndTime'].values\n",
    "        \n",
    "        # extract feature from bert\n",
    "        encoded = [tokenizer.encode(u, add_special_tokens=True) for u in utterance]\n",
    "        max_len = max([len(i) for i in encoded])\n",
    "        inputs_ids = torch.zeros([len(encoded),max_len]).long()\n",
    "        for i,e in enumerate(encoded):\n",
    "            inputs_ids[i,:len(e)-1]=torch.Tensor(e[:-1])\n",
    "            inputs_ids[i,-1]=e[-1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature = model(inputs_ids)[0][:,0,:]      \n",
    "        \n",
    "        # str to label\n",
    "        emotion_label = np.array([emotion_dict[e] for e in emotion])\n",
    "\n",
    "        # date to second\n",
    "        datetime_list = [datetime.strptime(t,'%H:%M:%S,%f') for t in startTime]\n",
    "        stime_list =  np.array([pt.second + pt.minute*60 + pt.hour*3600 + pt.microsecond*1e-6 for pt in datetime_list])\n",
    "        stime_list = stime_list - stime_list[0]\n",
    "\n",
    "        datetime_list = [datetime.strptime(t,'%H:%M:%S,%f') for t in endTime]\n",
    "        etime_list =  np.array([pt.second + pt.minute*60 + pt.hour*3600 + pt.microsecond*1e-6 for pt in datetime_list])\n",
    "        etime_list = etime_list - etime_list[0]\n",
    "\n",
    "        dataset.append((feature, emotion_label, stime_list, etime_list))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = 0\n",
    "category_list = []\n",
    "for i in df_train['Emotion']:\n",
    "    if i not in category_list:\n",
    "        category_list.append(i)\n",
    "category_list = sorted(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {}\n",
    "for i, c in enumerate(category_list):\n",
    "    emotion_dict[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = processed(df_train)\n",
    "dev_data = processed(df_dev)\n",
    "test_data = processed(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data, os.path.join('data/', 'train.pt'))\n",
    "torch.save(dev_data, os.path.join('data/', 'dev.pt'))\n",
    "torch.save(test_data, os.path.join('data/', 'test.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/MELD/raw/'\n",
    "train_text_avg_emb, val_text_avg_emb, test_text_avg_emb = pickle.load(open(root+'text_glove_average_emotion.pkl', 'rb'))\n",
    "df_train = pd.read_csv(root+'train_sent_emo.csv', index_col=0)\n",
    "df_dev = pd.read_csv(root+'dev_sent_emo.csv', index_col=0)\n",
    "df_test = pd.read_csv(root+'test_sent_emo.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed(df, text_avg_emb):\n",
    "    global emotion_dict\n",
    "    df_group = df.groupby('Dialogue_ID')\n",
    "    \n",
    "    dataset = []\n",
    "    for name, group in df_group:\n",
    "        #utterance = group['Utterance'].values\n",
    "        emotion = group['Emotion'].values\n",
    "        startTime = group['StartTime'].values\n",
    "        endTime = group['EndTime'].values\n",
    "        \n",
    "        dia_ID = group['Dialogue_ID'].values\n",
    "        utt_ID = group['Utterance_ID'].values\n",
    "        # extract feature from glove\n",
    "        feature = []\n",
    "        for d, u in zip(dia_ID, utt_ID):\n",
    "            key = str(d)+'_'+str(u)\n",
    "            feature.append(torch.tensor(text_avg_emb[key]))\n",
    "        feature = torch.stack(feature,0)\n",
    "        # str to label\n",
    "        emotion_label = torch.tensor([emotion_dict[e] for e in emotion])\n",
    "\n",
    "        # date to second\n",
    "        datetime_list = [datetime.strptime(t,'%H:%M:%S,%f') for t in startTime]\n",
    "        stime_list =  torch.tensor([pt.second + pt.minute*60 + pt.hour*3600 + pt.microsecond*1e-6 for pt in datetime_list])\n",
    "        bias = stime_list[0]\n",
    "        stime_list = stime_list - bias\n",
    "\n",
    "        datetime_list = [datetime.strptime(t,'%H:%M:%S,%f') for t in endTime]\n",
    "        etime_list =  torch.tensor([pt.second + pt.minute*60 + pt.hour*3600 + pt.microsecond*1e-6 for pt in datetime_list])\n",
    "        etime_list = etime_list - bias\n",
    "\n",
    "        dataset.append((feature, emotion_label, stime_list, etime_list))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = 0\n",
    "category_list = []\n",
    "for i in df_train['Emotion']:\n",
    "    if i not in category_list:\n",
    "        category_list.append(i)\n",
    "category_list = sorted(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {}\n",
    "for i, c in enumerate(category_list):\n",
    "    emotion_dict[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = processed(df_train, train_text_avg_emb)\n",
    "dev_data = processed(df_dev, val_text_avg_emb)\n",
    "test_data = processed(df_test, test_text_avg_emb)\n",
    "\n",
    "data = {\n",
    "    'train_data': train_data,\n",
    "    'dev_data': dev_data,\n",
    "    'test_data': test_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, os.path.join('data/MELD/processed', 'data.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'joy': 3,\n",
       " 'neutral': 4,\n",
       " 'sadness': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0002,  0.0934, -0.0510,  ...,  0.0136,  0.0032,  0.0425],\n",
       "         [-0.0026, -0.0166, -0.0269,  ..., -0.0131,  0.0370, -0.0158],\n",
       "         [ 0.0087,  0.0417, -0.0417,  ...,  0.0118,  0.0323,  0.0345],\n",
       "         ...,\n",
       "         [-0.0287,  0.1645, -0.1817,  ..., -0.0631,  0.0778,  0.0809],\n",
       "         [-0.0113,  0.0207, -0.0006,  ..., -0.0075,  0.0066,  0.0108],\n",
       "         [ 0.0092,  0.0202, -0.0338,  ..., -0.0031,  0.0223, -0.0009]]),\n",
       " tensor([4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 2, 4, 6, 4]),\n",
       " tensor([ 0.0000,  5.8810,  7.3830, 10.7610, 18.3930, 25.0670, 32.7410, 32.7410,\n",
       "         43.4180, 44.4190, 46.7970, 48.9660, 57.4319, 61.5200]),\n",
       " tensor([ 5.6720,  7.3830, 10.3300, 13.5130, 24.8580, 28.2780, 35.8270, 38.4550,\n",
       "         44.4190, 46.6600, 48.7990, 57.2650, 60.4770, 64.6480]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
